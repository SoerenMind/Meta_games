"""Experiments with functional-form agents."""
import itertools
import math
import typing

import numpy as np
import torch

from metagames import game
from . import agents
from . import losses


def scaled_normal_initializer(rand, num_parameters):
    """Initialize from a normal distribution scaled to give norm near 1."""
    return rand.normal(scale=1 / math.sqrt(num_parameters), size=(num_parameters,))


class PlayerSpecification(typing.NamedTuple):
    """A player specification for an experiment.

    Attributes:
        agent: The player agent. Multiple players may share the same agent.
        initializer: Parameter initializer function.
            Maps (numpy_random_state, num_parameters) -> A numpy array of size (num_parameters,)
        loss: A callable that returns the agent's loss on a game outcome.
        learning_rate: The players's learning rate.
        optimizer: The player's optimizer class.
        step_rate: Number of times the player is updated for each global step.
    """

    agent: agents.OpenSourceBinaryGameAgent
    initializer: typing.Any = scaled_normal_initializer
    loss: losses.BaseLoss = losses.UtilityLoss
    learning_rate: float = None
    optimizer: typing.Type[torch.optim.Optimizer] = torch.optim.SGD
    step_rate: int = 1


class _ExperimentPlayer(typing.NamedTuple):
    """An initialized experiment player."""

    spec: PlayerSpecification
    parameters: torch.Tensor
    optimizer: torch.optim.Optimizer


class Experiment:
    """Base experiment runner class."""

    def __init__(self, payoff_matrix, dtype=torch.double):
        self.payoff_matrix = payoff_matrix

    def _gen_steps(self, player_opponents, max_steps=None):
        """Generate experiment steps.

        Args:
            player_opponents: A list of (player, opponent_function) pairs.
                An opponent function takes the current step index and
                outputs an iterable of opponent players.
            max_steps: Optional maximum number of steps to run.

        Yields:
            For each step, a dictionary of step statistics.
        """
        if max_steps:
            steps = range(max_steps)
        else:
            steps = itertools.count()

        for step in steps:
            step_statistics = {"player_updates": []}
            for player, opponent_fn in player_opponents:
                opponents = opponent_fn(step)
                player_update_statistics = self._update_player(player, opponents)
                step_statistics["player_updates"].append(player_update_statistics)
            yield step_statistics

    def _initialize_players(player_specifications, seed=None):
        """Initialize players from a list of player specifications.

        Args:
            player_specifications: The player speceifications.
                An iterable of _ExperimentPlayer instances.
            seed: Optional player parameter initialization seed.
                The per-player seed depends on this seed and their order in `player_specifications`.
        """

        rand = np.random.RandomState(seed)

        players = []
        for player_spec in player_specifications:
            # Create a per-player random generator to so that player parameters
            # do not depend on the number of random samples generated by other players.
            player_rand = np.random.RandomState(rand.randint())
            parameters = player_spec.initializer(player_rand, player_spec.agent.num_parameters)

            optimizer_kwargs = {}
            if player_spec.learning_rate is not None:
                optimizer_kwargs["lr"] = player_spec.learning_rate

            optimizer = player_spec.optimizer([parameters], **optimizer_kwargs)
            players.append(_ExperimentPlayer(spec=player_spec, optimizer=optimizer, parameters=parameters))
        return players

    def play_game(self, agent, parameters, opponent_agent, opponent_parameters):
        """Play a game against an opponent."""
        action_logit = agent(parameters, opponent_parameters)
        opponent_action_logit = opponent_agent(opponent_parameters, parameters)

        action_probability = torch.sigmoid(action_logit)
        opponent_action_probability = torch.sigmoid(opponent_action_logit)
        utility = game.binary_game(self.payoff_matrix, action_probability, opponent_action_probability)
        return {"utility": utility, "action_logit": action_logit, "opponent_action_logit": opponent_action_logit}

    def _play_game_grad(self, agent, parameters, loss_fn, opponent_agent, opponent_parameters):
        """Play against an opponent and update player gradients."""
        results = self.play_game(agent, parameters, opponent_agent, opponent_parameters)
        statistics = {name: _tensor_data(value) for name, value in results.items()}

        loss = loss_fn(**results, parameter_vector=parameters, opponent_parameter_vector=opponent_parameters)

        loss.backward()

        return statistics

    def _update_player(self, player, opponents, update_steps=None):
        """Update a player from plays against a set of opponents.

        Args:
            player: The player to update. An instance of _ExperimentPlayer.
            opponents: The opponents. An iterable of _ExperimentPlayer.
            update_steps: Override the number of gradient descent steps performed.
                By default, uses player.spec.step_rate.
        """
        if update_steps is None:
            update_steps = player.spec.step_rate

        agent = player.spec.agent
        parameters = player.parameters
        optimizer = player.optimizer
        loss_fn = player.spec.loss

        statistics = []
        for _ in range(update_steps):
            optimizer.zero_grad()

            round_statistics = {"games": []}
            for opponent in opponents:
                game_statistics = self._play_game_grad(
                    agent=agent,
                    parameters=parameters,
                    loss_fn=loss_fn,
                    opponent_agent=opponent.spec.agent,
                    opponent_parameters=opponent.parameters,
                )
                round_statistics["games"].append({opponent: opponent.spec, **game_statistics})

            round_statistics["grad_norm"] = _tensor_data(torch.norm(parameters.grad))
            statistics.append(round_statistics)
            optimizer.step()

        return statistics


class SelfPlayExperiment(Experiment):
    def gen_steps(self, player_specification, self_aware, seed=None, max_steps=None):
        """Generator of player updates."""
        player, = self._initialize_players((player_specification,), seed=seed)
        if self_aware:
            opponent = player
        else:
            # Note: The detached version is a reference that has the same value as the original
            # parameters, so it is unnecessary to update on every gradient step.
            opponent = player._replace(parameters=player.parameters.detach())

        player_opponents = [(player, lambda step: (opponent,))]
        yield from self._gen_steps(player_opponents, max_steps=max_steps)

    def run(self, player_specification, num_steps, self_aware, seed=None):
        return list(self.gen_steps(player_specification, self_aware, seed=seed, max_steps=num_steps))


def _tensor_data(tensor):
    """The contents of a torch tensor as a numpy array."""
    return tensor.detach().cpu().numpy()
